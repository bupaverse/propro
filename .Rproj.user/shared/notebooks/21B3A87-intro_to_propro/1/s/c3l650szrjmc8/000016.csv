"0","propro %>%"
"0","    write_propro(""propro_model2.txt"") %>%"
"0","    run_propro(n.chains = 2, n.iter = 40000, n.burnin = 1000)"
"2","module glm loaded
"
"1","Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 5
   Total graph size: 33

"
"1","Initializing model

"
"1","  |                                                        "
"1","  |                                                  |   0%"
"1","  |                                                        "
"1","  |++++++++++++++++++++++++++++++++++++++++          |  80%"
"1","  |                                                        "
"1","  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%"
"1","
"
"1","  |                                                        "
"1","  |                                                  |   0%"
"1","  |                                                        "
"1","  |*                                                 |   2%"
"1","  |                                                        "
"1","  |**                                                |   4%"
"1","  |                                                        "
"1","  |***                                               |   6%"
"1","  |                                                        "
"1","  |****                                              |   8%"
"1","  |                                                        "
"1","  |*****                                             |  10%"
"1","  |                                                        "
"1","  |******                                            |  12%"
"1","  |                                                        "
"1","  |*******                                           |  14%"
"1","  |                                                        "
"1","  |********                                          |  16%"
"1","  |                                                        "
"1","  |*********                                         |  18%"
"1","  |                                                        "
"1","  |**********                                        |  21%"
"1","  |                                                        "
"1","  |***********                                       |  23%"
"1","  |                                                        "
"1","  |************                                      |  25%"
"1","  |                                                        "
"1","  |*************                                     |  27%"
"1","  |                                                        "
"1","  |**************                                    |  29%"
"1","  |                                                        "
"1","  |***************                                   |  31%"
"1","  |                                                        "
"1","  |****************                                  |  33%"
"1","  |                                                        "
"1","  |*****************                                 |  35%"
"1","  |                                                        "
"1","  |******************                                |  37%"
"1","  |                                                        "
"1","  |*******************                               |  39%"
"1","  |                                                        "
"1","  |*********************                             |  41%"
"1","  |                                                        "
"1","  |**********************                            |  43%"
"1","  |                                                        "
"1","  |***********************                           |  45%"
"1","  |                                                        "
"1","  |************************                          |  47%"
"1","  |                                                        "
"1","  |*************************                         |  49%"
"1","  |                                                        "
"1","  |**************************                        |  51%"
"1","  |                                                        "
"1","  |***************************                       |  53%"
"1","  |                                                        "
"1","  |****************************                      |  55%"
"1","  |                                                        "
"1","  |*****************************                     |  57%"
"1","  |                                                        "
"1","  |******************************                    |  59%"
"1","  |                                                        "
"1","  |*******************************                   |  62%"
"1","  |                                                        "
"1","  |********************************                  |  64%"
"1","  |                                                        "
"1","  |*********************************                 |  66%"
"1","  |                                                        "
"1","  |**********************************                |  68%"
"1","  |                                                        "
"1","  |***********************************               |  70%"
"1","  |                                                        "
"1","  |************************************              |  72%"
"1","  |                                                        "
"1","  |*************************************             |  74%"
"1","  |                                                        "
"1","  |**************************************            |  76%"
"1","  |                                                        "
"1","  |***************************************           |  78%"
"1","  |                                                        "
"1","  |****************************************          |  80%"
"1","  |                                                        "
"1","  |*****************************************         |  82%"
"1","  |                                                        "
"1","  |******************************************        |  84%"
"1","  |                                                        "
"1","  |*******************************************       |  86%"
"1","  |                                                        "
"1","  |********************************************      |  88%"
"1","  |                                                        "
"1","  |*********************************************     |  90%"
"1","  |                                                        "
"1","  |**********************************************    |  92%"
"1","  |                                                        "
"1","  |***********************************************   |  94%"
"1","  |                                                        "
"1","  |************************************************  |  96%"
"1","  |                                                        "
"1","  |************************************************* |  98%"
"1","  |                                                        "
"1","  |**************************************************| 100%"
"1","
"
"1","Inference for Bugs model at """
"1","propro_model2.txt"
"1",""", "
"1","fit using "
"1","jags"
"1",","
"1","
 "
"1","2"
"1"," chains, each with "
"1","40000"
"1"," iterations (first "
"1","1000"
"1"," discarded)"
"1",", n.thin ="
"1"," "
"1","39"
"1","
 n.sims ="
"1"," "
"1","2000"
"1"," "
"1","iterations saved
"
"1","         "
"1"," mu.vect"
"1"," sd.vect"
"1","   2.5%"
"1","    25%"
"1","    50%"
"1","    75%"
"1","  97.5%"
"1","  Rhat"
"1"," n.eff"
"1","
beta[10] "
"1","   0.257"
"1","   0.075"
"1","  0.127"
"1","  0.202"
"1","  0.250"
"1","  0.307"
"1","  0.414"
"1"," 1.001"
"1","  2000"
"1","
beta[11] "
"1","   0.743"
"1","   0.075"
"1","  0.586"
"1","  0.693"
"1","  0.750"
"1","  0.798"
"1","  0.873"
"1"," 1.001"
"1","  2000"
"1","
beta[12] "
"1","   0.548"
"1","   0.071"
"1","  0.407"
"1","  0.499"
"1","  0.550"
"1","  0.597"
"1","  0.685"
"1"," 1.001"
"1","  2000"
"1","
beta[13] "
"1","   0.452"
"1","   0.071"
"1","  0.315"
"1","  0.403"
"1","  0.450"
"1","  0.501"
"1","  0.593"
"1"," 1.001"
"1","  2000"
"1","
beta[14] "
"1","   0.257"
"1","   0.075"
"1","  0.127"
"1","  0.202"
"1","  0.250"
"1","  0.307"
"1","  0.414"
"1"," 1.001"
"1","  2000"
"1","
beta[15] "
"1","   0.743"
"1","   0.075"
"1","  0.586"
"1","  0.693"
"1","  0.750"
"1","  0.798"
"1","  0.873"
"1"," 1.001"
"1","  2000"
"1","
beta[16] "
"1","   0.331"
"1","   0.094"
"1","  0.162"
"1","  0.265"
"1","  0.329"
"1","  0.393"
"1","  0.522"
"1"," 1.001"
"1","  2000"
"1","
beta[17] "
"1","   0.459"
"1","   0.099"
"1","  0.267"
"1","  0.389"
"1","  0.459"
"1","  0.530"
"1","  0.652"
"1"," 1.002"
"1","  2000"
"1","
beta[18] "
"1","   0.210"
"1","   0.082"
"1","  0.079"
"1","  0.150"
"1","  0.201"
"1","  0.259"
"1","  0.392"
"1"," 1.001"
"1","  2000"
"1","
beta[19] "
"1","   0.257"
"1","   0.075"
"1","  0.127"
"1","  0.202"
"1","  0.250"
"1","  0.307"
"1","  0.414"
"1"," 1.001"
"1","  2000"
"1","
beta[1]  "
"1","   0.473"
"1","   0.072"
"1","  0.328"
"1","  0.425"
"1","  0.472"
"1","  0.521"
"1","  0.616"
"1"," 1.001"
"1","  2000"
"1","
beta[20] "
"1","   0.743"
"1","   0.075"
"1","  0.586"
"1","  0.693"
"1","  0.750"
"1","  0.798"
"1","  0.873"
"1"," 1.001"
"1","  2000"
"1","
beta[2]  "
"1","   0.527"
"1","   0.072"
"1","  0.384"
"1","  0.479"
"1","  0.528"
"1","  0.575"
"1","  0.672"
"1"," 1.001"
"1","  2000"
"1","
beta[3]  "
"1","   0.548"
"1","   0.071"
"1","  0.407"
"1","  0.499"
"1","  0.550"
"1","  0.597"
"1","  0.685"
"1"," 1.001"
"1","  2000"
"1","
beta[4]  "
"1","   0.452"
"1","   0.071"
"1","  0.315"
"1","  0.403"
"1","  0.450"
"1","  0.501"
"1","  0.593"
"1"," 1.001"
"1","  2000"
"1","
beta[5]  "
"1","   0.257"
"1","   0.075"
"1","  0.127"
"1","  0.202"
"1","  0.250"
"1","  0.307"
"1","  0.414"
"1"," 1.001"
"1","  2000"
"1","
beta[6]  "
"1","   0.743"
"1","   0.075"
"1","  0.586"
"1","  0.693"
"1","  0.750"
"1","  0.798"
"1","  0.873"
"1"," 1.001"
"1","  2000"
"1","
beta[7]  "
"1","   0.331"
"1","   0.094"
"1","  0.162"
"1","  0.265"
"1","  0.329"
"1","  0.393"
"1","  0.522"
"1"," 1.001"
"1","  2000"
"1","
beta[8]  "
"1","   0.459"
"1","   0.099"
"1","  0.267"
"1","  0.389"
"1","  0.459"
"1","  0.530"
"1","  0.652"
"1"," 1.002"
"1","  2000"
"1","
beta[9]  "
"1","   0.210"
"1","   0.082"
"1","  0.079"
"1","  0.150"
"1","  0.201"
"1","  0.259"
"1","  0.392"
"1"," 1.001"
"1","  2000"
"1","
beta_f   "
"1","   0.927"
"1","   0.034"
"1","  0.849"
"1","  0.906"
"1","  0.930"
"1","  0.952"
"1","  0.980"
"1"," 1.003"
"1","   630"
"1","
theta[10]"
"1","   0.062"
"1","   0.022"
"1","  0.027"
"1","  0.045"
"1","  0.059"
"1","  0.075"
"1","  0.113"
"1"," 1.001"
"1","  2000"
"1","
theta[11]"
"1","   0.073"
"1","   0.034"
"1","  0.020"
"1","  0.048"
"1","  0.070"
"1","  0.094"
"1","  0.151"
"1"," 1.005"
"1","   480"
"1","
theta[12]"
"1","   0.037"
"1","   0.016"
"1","  0.013"
"1","  0.025"
"1","  0.034"
"1","  0.046"
"1","  0.077"
"1"," 1.003"
"1","  2000"
"1","
theta[1] "
"1","   0.046"
"1","   0.021"
"1","  0.016"
"1","  0.031"
"1","  0.043"
"1","  0.059"
"1","  0.096"
"1"," 1.001"
"1","  2000"
"1","
theta[2] "
"1","   0.101"
"1","   0.032"
"1","  0.050"
"1","  0.079"
"1","  0.098"
"1","  0.120"
"1","  0.172"
"1"," 1.001"
"1","  2000"
"1","
theta[3] "
"1","   0.054"
"1","   0.021"
"1","  0.023"
"1","  0.039"
"1","  0.051"
"1","  0.066"
"1","  0.102"
"1"," 1.001"
"1","  2000"
"1","
theta[4] "
"1","   0.199"
"1","   0.043"
"1","  0.125"
"1","  0.167"
"1","  0.197"
"1","  0.226"
"1","  0.292"
"1"," 1.001"
"1","  2000"
"1","
theta[5] "
"1","   0.069"
"1","   0.025"
"1","  0.031"
"1","  0.051"
"1","  0.065"
"1","  0.083"
"1","  0.125"
"1"," 1.001"
"1","  2000"
"1","
theta[6] "
"1","   0.042"
"1","   0.019"
"1","  0.014"
"1","  0.028"
"1","  0.038"
"1","  0.052"
"1","  0.085"
"1"," 1.002"
"1","  2000"
"1","
theta[7] "
"1","   0.091"
"1","   0.028"
"1","  0.044"
"1","  0.070"
"1","  0.088"
"1","  0.109"
"1","  0.153"
"1"," 1.001"
"1","  2000"
"1","
theta[8] "
"1","   0.049"
"1","   0.019"
"1","  0.020"
"1","  0.035"
"1","  0.046"
"1","  0.059"
"1","  0.092"
"1"," 1.001"
"1","  2000"
"1","
theta[9] "
"1","   0.178"
"1","   0.041"
"1","  0.106"
"1","  0.149"
"1","  0.175"
"1","  0.205"
"1","  0.264"
"1"," 1.001"
"1","  1800"
"1","
deviance "
"1","  42.285"
"1","   3.305"
"1"," 37.734"
"1"," 39.851"
"1"," 41.618"
"1"," 44.068"
"1"," 50.249"
"1"," 1.004"
"1","   420"
"1","
"
"1","
For each parameter, n.eff is a crude measure of effective sample size,"
"1","
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
"
"1","
DIC info (using the rule, pD = var(deviance)/2)
"
"1","pD ="
"1"," "
"1","5.5"
"1"," "
"1","and DIC ="
"1"," "
"1","47.7"
"1","
DIC is an estimate of expected predictive error (lower deviance is better).
"
